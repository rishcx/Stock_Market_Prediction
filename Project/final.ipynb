{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name=input(\"Enter Stock Name: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.parse import unquote\n",
    "from scipy.special import softmax\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_article(url):\n",
    "    headers = {\n",
    "    'User-Agent': 'Your User Agent String',\n",
    "    }\n",
    "    r=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(r.text,'html.parser')\n",
    "    paragraphs=soup.find_all('p')\n",
    "    text= [paragraph.text for paragraph in paragraphs]\n",
    "    words=' '.join(text).split(' ')\n",
    "    article = ' '.join(words)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url(keyword):\n",
    "    \n",
    "    root = \"https://www.google.com/\"\n",
    "    search_query = keyword.replace(\" \", \"+\")\n",
    "    link = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(link, headers=headers)\n",
    "    webpage = response.content\n",
    "    soup = BeautifulSoup(webpage, 'html5lib')\n",
    "    links = []\n",
    "    for div_tag in soup.find_all('div', class_='Gx5Zad'):\n",
    "        a_tag = div_tag.find('a')\n",
    "        if a_tag:\n",
    "            if 'href' in a_tag.attrs:\n",
    "                href = a_tag['href']\n",
    "                if href.startswith('/url?q='):\n",
    "                    url = href.split('/url?q=')[1].split('&sa=')[0]\n",
    "                    links.append(url)\n",
    "    return links    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_chunks(data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=3000,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    docs=text_splitter.split_text(data)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bart_model(model_name=\"facebook/bart-large-cnn\"):\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_news_url(keyword, start_date, end_date):\n",
    "    root = \"https://www.google.com/\"\n",
    "    search_query = keyword.replace(\" \", \"+\")\n",
    "    link = f\"{root}search?q={search_query}&tbm=nws&tbs=cdr:1,cd_min:{start_date},cd_max:{end_date}\"\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "\n",
    "    response = requests.get(link, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    news_links = []\n",
    "\n",
    "    for article in soup.select('div.SoaBEf'):\n",
    "        link = article.select_one('a')\n",
    "        if link and 'href' in link.attrs:\n",
    "            url = link['href']\n",
    "            if url.startswith('/url?q='):\n",
    "                url = unquote(url.split('/url?q=')[1].split('&sa=')[0])\n",
    "            news_links.append(url)\n",
    "\n",
    "    return news_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(tokenizer, model, text, max_chunk_length, summary_max_length):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_chunk_length, truncation=True)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=summary_max_length, min_length=200, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_article(url, model_name=\"facebook/bart-large-cnn\"):\n",
    "    data = scraping_article(url)\n",
    "    chunks = to_chunks(data)\n",
    "    # tokenizer, model=load_pegasus_model(\"google/pegasus-xsum\")\n",
    "    tokenizer, model = load_bart_model(model_name)\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk\n",
    "        summary = summarize_text(tokenizer, model, chunk_text,3000,800)\n",
    "        summaries.append(summary)\n",
    "    concatenated_summaries = \" \".join(summaries)\n",
    "    #  Second summarization pass: Summarize the concatenated summaries\n",
    "    intermediate_chunks = [concatenated_summaries[i:i+3000] for i in range(0, len(concatenated_summaries), 3000)]\n",
    "    final_summaries = []\n",
    "    for intermediate_chunk in intermediate_chunks:\n",
    "        final_summary = summarize_text(tokenizer, model, intermediate_chunk,3000,800)\n",
    "        final_summaries.append(final_summary)    \n",
    "    final_summary_text = \" \".join(final_summaries)    \n",
    "    return final_summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti_model(model_name=\"cardiffnlp/twitter-roberta-base-sentiment\"):\n",
    "    tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    return tokenizer,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_senti(news_texts):\n",
    "    tokenizer,model=senti_model()\n",
    "    encoded=tokenizer(news_texts,return_tensors='pt',truncation=True, padding=True, max_length=512)\n",
    "    output=model(**encoded)\n",
    "    scores=output[0][0].detach().numpy()\n",
    "    scores=softmax(scores)\n",
    "    weights = {\n",
    "        'neg': -1,\n",
    "        'neu': 0,\n",
    "        'pos': 1\n",
    "    }\n",
    "    probabilities = {\n",
    "        'neg': scores[0],\n",
    "        'neu': scores[1],\n",
    "        'pos': scores[2]\n",
    "    }\n",
    "    compound_score = sum(probabilities[label] * weights[label] for label in probabilities)\n",
    "    senti_dict={\n",
    "        'neg':scores[0],\n",
    "        'neu': scores[1],\n",
    "        'pos': scores[2],\n",
    "        'polarity':compound_score        \n",
    "    }\n",
    "    return senti_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(summary):\n",
    "    sentiment_scores = find_senti(summary)\n",
    "    features = {\n",
    "        'compound_sentiment_score': sentiment_scores['polarity'],  \n",
    "        'negative_sentiment_score': sentiment_scores['neg'],\n",
    "        'neutral_sentiment_score': sentiment_scores['neu'],\n",
    "        'positive_sentiment_score': sentiment_scores['pos']\n",
    "    }    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=find_url(stock_name)\n",
    "summaries=[]\n",
    "for i in range(5):\n",
    "    summary=summarize_article(urls[i])\n",
    "    summaries.append(summary)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla's older lineup of vehicles is having a harder time keeping up with fresher offerings from rival EV manufacturers, an analyst says. The company has not revealed any details about the cars it plans to introduce next year but has said it will have a new model by the end of the year. Musk received a major vote of confidence last month, when shareholders voted in favor of re-approving his $56 billion compensation package. The carmaker is expected to deliver 435,200 units this quarter and about 1.83 million for the year, up only slightly from its 2023 total, a Robert W. Baird analyst said last week. It's tougher to grow when you have increased competition and the current model lineup is a little stale,\" Tom Narayan, a global autos analyst at RBC Capital Markets, said in a recent report.  â€œWe see a growing number of investors shifting their outlook to the Robotaxi event on Aug. 8,\" Ben Kallo, a RBC analyst, wrote in a report.\n"
     ]
    }
   ],
   "source": [
    "print(summaries[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utkar\\Desktop\\DG_liger\\Project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "all_scores=[]\n",
    "for i in range(5):\n",
    "    scores=extract_features(summaries[i])\n",
    "    all_scores.append(scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score={}\n",
    "avg_comp=0\n",
    "avg_pos=0\n",
    "avg_neg=0\n",
    "avg_neu=0\n",
    "for i in range(5):\n",
    "    avg_comp+=all_scores[i][\"compound_sentiment_score\"]\n",
    "    avg_neg+=all_scores[i][\"negative_sentiment_score\"]\n",
    "    avg_neu+=all_scores[i][\"neutral_sentiment_score\"]\n",
    "    avg_pos+=all_scores[i][\"positive_sentiment_score\"]\n",
    "avg_score[\"avg_compound_score\"]=avg_comp/5\n",
    "avg_score[\"avg_negative_score\"]=avg_neg/5\n",
    "avg_score[\"avg_neutral_score\"]=avg_neu/5\n",
    "avg_score[\"avg_positive_score\"]=avg_pos/5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stock will go up on the basis of: \n",
      "https://www.ndtv.com/feature/girl-in-china-asks-elon-musk-to-fix-a-bug-on-her-tesla-screen-he-reacts-6018679\n",
      "https://www.livemint.com/market/stock-market-news/tesla-share-price-jumps-over-10-as-second-quarter-sales-beat-estimates-11719972897779.html\n",
      "https://www.moneycontrol.com/news/world/tesla-beats-estimates-with-less-drastic-drop-in-ev-sales-12761058.html\n",
      "https://m.economictimes.com/markets/stocks/news/tesla-shares-rally-over-10-as-quarterly-deliveries-beat-estimates/articleshow/111437591.cms\n",
      "https://auto.hindustantimes.com/auto/electric-vehicles/tesla-is-running-out-of-excuses-for-its-prolonged-sales-slump-41719834908586.html\n"
     ]
    }
   ],
   "source": [
    "if avg_score[\"avg_compound_score\"]>0:\n",
    "    print(\"The stock will go up on the basis of: \")\n",
    "    for i in range(5):\n",
    "        print(urls[i],end=\"\\n\")\n",
    "else:\n",
    "    print(\"The stock will go down\")\n",
    "    for i in range(5):\n",
    "        print(urls[i],end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
