{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utkar\\Desktop\\DG_liger\\Project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stock will go up based on the following articles:\n",
      "https://auto.hindustantimes.com/auto/cars/toyota-innova-hycross-mpv-booking-halted-for-top-end-hybrid-variants-again-41721029350408.html\n",
      "https://www.autocarindia.com/car-news/toyota-hyryder-waiting-period-down-to-three-months-432228\n",
      "https://motoroctane.com/news/275572-3-reasons-why-marutis-flop-is-toyotas-biggest-hit\n",
      "https://www.cartoq.com/car-news/toyota-launch-3-new-suvs-india-electric-hybrid-7-seater/\n",
      "https://www.ndtv.com/india-news/video-mahindra-scorpio-flips-on-collision-with-toyota-fortuner-in-haryana-6110686\n",
      "\n",
      "Average Sentiment Scores:\n",
      "avg_compound_score: 0.20314366267994047\n",
      "avg_negative_score: 0.10185616267845035\n",
      "avg_neutral_score: 0.5931440234184265\n",
      "avg_positive_score: 0.3049998253583908\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import unquote\n",
    "from scipy.special import softmax\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "def scraping_article(url):\n",
    "    headers = {\n",
    "    'User-Agent': 'Your User Agent String',\n",
    "    }\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, 'html5lib')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = [paragraph.text for paragraph in paragraphs]\n",
    "    words = ' '.join(text).split(' ')\n",
    "    article = ' '.join(words)\n",
    "    return article\n",
    "\n",
    "def find_url(keyword):\n",
    "    root = \"https://www.google.com/\"\n",
    "    search_query = keyword.replace(\" \", \"+\")\n",
    "    link = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(link, headers=headers)\n",
    "    webpage = response.content\n",
    "    soup = BeautifulSoup(webpage, 'html5lib')\n",
    "    links = []\n",
    "    for div_tag in soup.find_all('div', class_='Gx5Zad'):\n",
    "        a_tag = div_tag.find('a')\n",
    "        if a_tag:\n",
    "            if 'href' in a_tag.attrs:\n",
    "                href = a_tag['href']\n",
    "                if href.startswith('/url?q='):\n",
    "                    url = href.split('/url?q=')[1].split('&sa=')[0]\n",
    "                    links.append(url)\n",
    "    return links\n",
    "\n",
    "def to_chunks(data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=3000,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    docs = text_splitter.split_text(data)\n",
    "    return docs\n",
    "\n",
    "def load_bart_model(model_name=\"facebook/bart-large-cnn\"):\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def find_news_url(keyword, start_date, end_date):\n",
    "    root = \"https://www.google.com/\"\n",
    "    search_query = keyword.replace(\" \", \"+\")\n",
    "    link = f\"{root}search?q={search_query}&tbm=nws&tbs=cdr:1,cd_min:{start_date},cd_max:{end_date}\"\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "\n",
    "    response = requests.get(link, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    news_links = []\n",
    "\n",
    "    for article in soup.select('div.SoaBEf'):\n",
    "        link = article.select_one('a')\n",
    "        if link and 'href' in link.attrs:\n",
    "            url = link['href']\n",
    "            if url.startswith('/url?q='):\n",
    "                url = unquote(url.split('/url?q=')[1].split('&sa=')[0])\n",
    "            news_links.append(url)\n",
    "\n",
    "    return news_links\n",
    "\n",
    "def summarize_text(tokenizer, model, text, max_chunk_length, summary_max_length):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_chunk_length, truncation=True)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=summary_max_length, min_length=200, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def summarize_article(url, model_name=\"facebook/bart-large-cnn\"):\n",
    "    data = scraping_article(url)\n",
    "    chunks = to_chunks(data)\n",
    "    tokenizer, model = load_bart_model(model_name)\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk\n",
    "        summary = summarize_text(tokenizer, model, chunk_text, 3000, 800)\n",
    "        summaries.append(summary)\n",
    "    concatenated_summaries = \" \".join(summaries)\n",
    "    intermediate_chunks = [concatenated_summaries[i:i+3000] for i in range(0, len(concatenated_summaries), 3000)]\n",
    "    final_summaries = []\n",
    "    for intermediate_chunk in intermediate_chunks:\n",
    "        final_summary = summarize_text(tokenizer, model, intermediate_chunk, 3000, 800)\n",
    "        final_summaries.append(final_summary)    \n",
    "    final_summary_text = \" \".join(final_summaries)    \n",
    "    return final_summary_text\n",
    "\n",
    "def senti_model(model_name=\"cardiffnlp/twitter-roberta-base-sentiment\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def find_senti(news_texts):\n",
    "    tokenizer, model = senti_model()\n",
    "    encoded = tokenizer(news_texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    output = model(**encoded)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    weights = {\n",
    "        'neg': -1,\n",
    "        'neu': 0,\n",
    "        'pos': 1\n",
    "    }\n",
    "    probabilities = {\n",
    "        'neg': scores[0],\n",
    "        'neu': scores[1],\n",
    "        'pos': scores[2]\n",
    "    }\n",
    "    compound_score = sum(probabilities[label] * weights[label] for label in probabilities)\n",
    "    senti_dict = {\n",
    "        'neg': scores[0],\n",
    "        'neu': scores[1],\n",
    "        'pos': scores[2],\n",
    "        'polarity': compound_score        \n",
    "    }\n",
    "    return senti_dict\n",
    "\n",
    "def extract_features(summary):\n",
    "    sentiment_scores = find_senti(summary)\n",
    "    features = {\n",
    "        'compound_sentiment_score': sentiment_scores['polarity'],  \n",
    "        'negative_sentiment_score': sentiment_scores['neg'],\n",
    "        'neutral_sentiment_score': sentiment_scores['neu'],\n",
    "        'positive_sentiment_score': sentiment_scores['pos']\n",
    "    }    \n",
    "    return features\n",
    "\n",
    "def analyze_stock(stock_name):\n",
    "    urls = find_url(stock_name)\n",
    "    summaries = []\n",
    "    for i in range(5):\n",
    "        summary = summarize_article(urls[i])\n",
    "        summaries.append(summary)\n",
    "\n",
    "    all_scores = []\n",
    "    for i in range(5):\n",
    "        scores = extract_features(summaries[i])\n",
    "        all_scores.append(scores)\n",
    "\n",
    "    avg_score = {}\n",
    "    avg_comp = 0\n",
    "    avg_pos = 0\n",
    "    avg_neg = 0\n",
    "    avg_neu = 0\n",
    "    for i in range(5):\n",
    "        avg_comp += all_scores[i][\"compound_sentiment_score\"]\n",
    "        avg_neg += all_scores[i][\"negative_sentiment_score\"]\n",
    "        avg_neu += all_scores[i][\"neutral_sentiment_score\"]\n",
    "        avg_pos += all_scores[i][\"positive_sentiment_score\"]\n",
    "    avg_score[\"avg_compound_score\"] = avg_comp / 5\n",
    "    avg_score[\"avg_negative_score\"] = avg_neg / 5\n",
    "    avg_score[\"avg_neutral_score\"] = avg_neu / 5\n",
    "    avg_score[\"avg_positive_score\"] = avg_pos / 5\n",
    "\n",
    "    result = \"\"\n",
    "    if avg_score[\"avg_compound_score\"] > 0:\n",
    "        result = \"The stock will go up based on the following articles:\\n\"\n",
    "    else:\n",
    "        result = \"The stock will go down based on the following articles:\\n\"\n",
    "\n",
    "    for url in urls[:5]:\n",
    "        result += f\"{url}\\n\"\n",
    "\n",
    "    return result, avg_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stock_name = input(\"Enter Stock Name: \")\n",
    "    result, avg_score = analyze_stock(stock_name)\n",
    "    print(result)\n",
    "    print(\"Average Sentiment Scores:\")\n",
    "    for key, value in avg_score.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model Accuracy: 0.5238095238095238\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.57      0.44         7\n",
      "           1       0.70      0.50      0.58        14\n",
      "\n",
      "    accuracy                           0.52        21\n",
      "   macro avg       0.53      0.54      0.51        21\n",
      "weighted avg       0.59      0.52      0.54        21\n",
      "\n",
      "\n",
      "Overall XGBoost Model Accuracy: 0.52\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 207\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOverall XGBoost Model Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    206\u001b[0m stock_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEnter Stock Name: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 207\u001b[0m result, avg_score, urls, summaries \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_stock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[1;32mIn[28], line 166\u001b[0m, in \u001b[0;36manalyze_stock\u001b[1;34m(stock_name)\u001b[0m\n\u001b[0;32m    164\u001b[0m summaries \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m--> 166\u001b[0m     summary \u001b[38;5;241m=\u001b[39m summarize_article(\u001b[43murls\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    167\u001b[0m     summaries\u001b[38;5;241m.\u001b[39mappend(summary)\n\u001b[0;32m    169\u001b[0m all_scores \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import unquote\n",
    "from scipy.special import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BartTokenizer, BartForConditionalGeneration\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "def scraping_article(url):\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    }\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = [paragraph.text for paragraph in paragraphs]\n",
    "    words = ' '.join(text).split(' ')\n",
    "    article = ' '.join(words)\n",
    "    return article\n",
    "\n",
    "def find_url(keyword):\n",
    "    root = \"https://www.google.com/\"\n",
    "    search_query = keyword.replace(\" \", \"+\")\n",
    "    link = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    response = requests.get(link, headers=headers)\n",
    "    webpage = response.content\n",
    "    soup = BeautifulSoup(webpage, 'html5lib')\n",
    "    links = []\n",
    "    for div_tag in soup.find_all('div', class_='Gx5Zad'):\n",
    "        a_tag = div_tag.find('a')\n",
    "        if a_tag:\n",
    "            if 'href' in a_tag.attrs:\n",
    "                href = a_tag['href']\n",
    "                if href.startswith('/url?q='):\n",
    "                    url = href.split('/url?q=')[1].split('&sa=')[0]\n",
    "                    links.append(url)\n",
    "    return links\n",
    "\n",
    "def to_chunks(data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=3000,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    docs = text_splitter.split_text(data)\n",
    "    return docs\n",
    "\n",
    "def load_bart_model(model_name=\"facebook/bart-large-cnn\"):\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def summarize_text(tokenizer, model, text, max_chunk_length, summary_max_length):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_chunk_length, truncation=True)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=summary_max_length, min_length=200, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def summarize_article(url, model_name=\"facebook/bart-large-cnn\"):\n",
    "    data = scraping_article(url)\n",
    "    chunks = to_chunks(data)\n",
    "    tokenizer, model = load_bart_model(model_name)\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk\n",
    "        summary = summarize_text(tokenizer, model, chunk_text, 3000, 800)\n",
    "        summaries.append(summary)\n",
    "    concatenated_summaries = \" \".join(summaries)\n",
    "    intermediate_chunks = [concatenated_summaries[i:i+3000] for i in range(0, len(concatenated_summaries), 3000)]\n",
    "    final_summaries = []\n",
    "    for intermediate_chunk in intermediate_chunks:\n",
    "        final_summary = summarize_text(tokenizer, model, intermediate_chunk, 3000, 800)\n",
    "        final_summaries.append(final_summary)    \n",
    "    final_summary_text = \" \".join(final_summaries)    \n",
    "    return final_summary_text\n",
    "\n",
    "def senti_model(model_name=\"cardiffnlp/twitter-roberta-base-sentiment\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def find_senti(news_texts):\n",
    "    tokenizer, model = senti_model()\n",
    "    encoded = tokenizer(news_texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    output = model(**encoded)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    weights = {\n",
    "        'neg': -1,\n",
    "        'neu': 0,\n",
    "        'pos': 1\n",
    "    }\n",
    "    probabilities = {\n",
    "        'neg': scores[0],\n",
    "        'neu': scores[1],\n",
    "        'pos': scores[2]\n",
    "    }\n",
    "    compound_score = sum(probabilities[label] * weights[label] for label in probabilities)\n",
    "    senti_dict = {\n",
    "        'neg': scores[0],\n",
    "        'neu': scores[1],\n",
    "        'pos': scores[2],\n",
    "        'polarity': compound_score        \n",
    "    }\n",
    "    return senti_dict\n",
    "\n",
    "def extract_features(summary):\n",
    "    sentiment_scores = find_senti(summary)\n",
    "    features = {\n",
    "        'compound_sentiment_score': sentiment_scores['polarity'],  \n",
    "        'negative_sentiment_score': sentiment_scores['neg'],\n",
    "        'neutral_sentiment_score': sentiment_scores['neu'],\n",
    "        'positive_sentiment_score': sentiment_scores['pos']\n",
    "    }    \n",
    "    return features\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df[['avg_compound_sentiment_score', 'avg_negative_sentiment_score', \n",
    "            'avg_neutral_sentiment_score', 'avg_positive_sentiment_score']]\n",
    "    y = df['Movement']\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def train_xgboost(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    params = {\n",
    "        'max_depth': 3,\n",
    "        'eta': 0.1,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "    \n",
    "    num_round = 100\n",
    "    xgb_model = xgb.train(params, dtrain, num_round)\n",
    "    \n",
    "    y_pred = xgb_model.predict(dtest)\n",
    "    y_pred_binary = [1 if y > 0.5 else 0 for y in y_pred]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    \n",
    "    print(\"XGBoost Model Accuracy:\", accuracy)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_binary))\n",
    "    \n",
    "    return xgb_model, accuracy\n",
    "\n",
    "def predict_stock_movement(xgb_model, features):\n",
    "    dfeatures = xgb.DMatrix([features])\n",
    "    prediction = xgb_model.predict(dfeatures)\n",
    "    return \"up\" if prediction[0] > 0.5 else \"down\"\n",
    "\n",
    "def analyze_stock(stock_name):\n",
    "    urls = find_url(stock_name)\n",
    "    summaries = []\n",
    "    for i in range(5):\n",
    "        summary = summarize_article(urls[i])\n",
    "        summaries.append(summary)\n",
    "\n",
    "    all_scores = []\n",
    "    for i in range(5):\n",
    "        scores = extract_features(summaries[i])\n",
    "        all_scores.append(scores)\n",
    "\n",
    "    avg_score = {}\n",
    "    avg_comp = 0\n",
    "    avg_pos = 0\n",
    "    avg_neg = 0\n",
    "    avg_neu = 0\n",
    "    for i in range(5):\n",
    "        avg_comp += all_scores[i][\"compound_sentiment_score\"]\n",
    "        avg_neg += all_scores[i][\"negative_sentiment_score\"]\n",
    "        avg_neu += all_scores[i][\"neutral_sentiment_score\"]\n",
    "        avg_pos += all_scores[i][\"positive_sentiment_score\"]\n",
    "    avg_score[\"avg_compound_score\"] = avg_comp / 5\n",
    "    avg_score[\"avg_negative_score\"] = avg_neg / 5\n",
    "    avg_score[\"avg_neutral_score\"] = avg_neu / 5\n",
    "    avg_score[\"avg_positive_score\"] = avg_pos / 5\n",
    "\n",
    "    result = \"Based on sentiment analysis:\\n\"\n",
    "    if avg_score[\"avg_compound_score\"] > 0:\n",
    "        result += \"The overall sentiment is positive.\\n\"\n",
    "    else:\n",
    "        result += \"The overall sentiment is negative.\\n\"\n",
    "\n",
    "    return result, avg_score, urls, summaries\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and prepare data\n",
    "    X, y = load_and_prepare_data(\"update.csv\")\n",
    "    \n",
    "    # Train XGBoost model\n",
    "    xgb_model, model_accuracy = train_xgboost(X, y)\n",
    "    \n",
    "    print(f\"\\nOverall XGBoost Model Accuracy: {model_accuracy:.2f}\")\n",
    "    \n",
    "    stock_name = input(\"\\nEnter Stock Name: \")\n",
    "    result, avg_score, urls, summaries = analyze_stock(stock_name)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(result)\n",
    "    print(\"Average Sentiment Scores:\")\n",
    "    for key, value in avg_score.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nArticle Summaries and URLs:\")\n",
    "    for i, (url, summary) in enumerate(zip(urls[:5], summaries), 1):\n",
    "        print(f\"\\nArticle {i}:\")\n",
    "        print(f\"URL: {url}\")\n",
    "        print(f\"Summary: {summary}\")\n",
    "    \n",
    "    # Prepare features for prediction\n",
    "    features = [\n",
    "        avg_score[\"avg_compound_score\"],\n",
    "        avg_score[\"avg_negative_score\"],\n",
    "        avg_score[\"avg_neutral_score\"],\n",
    "        avg_score[\"avg_positive_score\"]\n",
    "    ]\n",
    "    \n",
    "    # Predict stock movement using XGBoost\n",
    "    xgb_prediction = predict_stock_movement(xgb_model, features)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"XGBoost Prediction: Based on the current sentiment analysis and historical data,\")\n",
    "    print(f\"the stock is likely to go {xgb_prediction}.\")\n",
    "    print(f\"Model Accuracy: {model_accuracy:.2f}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url(keyword):\n",
    "    root = \"https://www.google.com/\"\n",
    "    search_query = keyword.replace(\" \", \"+\")\n",
    "    link = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    response = requests.get(link, headers=headers)\n",
    "    webpage = response.content\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "    links = []\n",
    "    for div_tag in soup.find_all('div', class_='Gx5Zad'):\n",
    "        a_tag = div_tag.find('a')\n",
    "        if a_tag:\n",
    "            if 'href' in a_tag.attrs:\n",
    "                href = a_tag['href']\n",
    "                if href.startswith('/url?q='):\n",
    "                    url = href.split('/url?q=')[1].split('&sa=')[0]\n",
    "                    links.append(url)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_url(\"tesla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
