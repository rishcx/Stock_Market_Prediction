{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_texts=\"\"\"Tesla announced it was venturing into auto insurance in 2019. It promised a better and cheaper insurance experience for electric vehicle (EV) drivers beset by high repair costs and premiums. But nearly four years since it launched, Tesla Insurance has faced significant challenges and questions over its viability. A slew of consumer complaints drew lawsuits and regulatory scrutiny last year, and the brakes appear to have been put on Tesla Insurance’s launch in Europe, originally slated for 2023. One analyst said the EV giant seems to have run into the same problems that other tech firms fall into while trying to enter insurance. But he also points out that increased scrutiny over Tesla Insurance could also be part of the “Elon Musk effect” “When I look at all this stuff around Tesla, and all the noise around it, I think some of it is just that Elon Musk is larger than life. People react to him, and they try to jump on any bad news,’ he said. \"It's causing them all kinds of public relations problems,\" Denninginger said. \"They’ll have to work their way out of it,\" he said of Tesla Insurance. \"It's not going to be easy, but it's not the end of the world,\" he concluded. \"There’s nothing new here from an insurance industry perspective,\" he added. “I’m sure they’re fixing it because the company is incredible when you think about what they�’ve done,’ he said.“I'm sure they're fixing it,” he said, “but in the meantime, it’d be nice if they didn’t do this.” “It’ s going to happen to all insurance companies at some point.’ “ “There's nothing new in the insurance industry from a policyholder perspective,“ Denninger said. � � “You’ have to get used to the idea that insurance companies are going to fail.�\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import yfinance as yf\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from scipy.special import softmax\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.044, 'neu': 0.876, 'pos': 0.08, 'compound': 0.8151}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "sia=SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(news_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utkar\\Desktop\\DG_liger\\Project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name=\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "model=AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.40104452 0.4877971  0.11115833]\n"
     ]
    }
   ],
   "source": [
    "encoded=tokenizer(news_texts,return_tensors='pt')\n",
    "output=model(**encoded)\n",
    "scores=output[0][0].detach().numpy()\n",
    "scores=softmax(scores)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compound score: -0.2898861840367317\n"
     ]
    }
   ],
   "source": [
    "weights = {\n",
    "    'neg': -1,\n",
    "    'neu': 0,\n",
    "    'pos': 1\n",
    "}\n",
    "probabilities = {\n",
    "    'neg': scores[0],\n",
    "    'neu': scores[1],\n",
    "    'pos': scores[2]\n",
    "}\n",
    "compound_score = sum(probabilities[label] * weights[label] for label in probabilities)\n",
    "print(f\"Compound score: {compound_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/search?q=elon+musk&tbs=cdr%3A1%2Ccd_min%3A6%2F8%2F2023%2Ccd_max%3A6%2F8%2F2023&tbm=nws\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def find_url(keyword):\n",
    "    # Convert dates to the format Google expects (M/D/YYYY)\n",
    "    start_date = \"6/8/2023\"\n",
    "    end_date = \"6/8/2023\"\n",
    "\n",
    "    search_query = keyword.replace(\" \", \"+\")\n",
    "    # Construct the URL with the tbs parameter for date range\n",
    "    link = f\"https://www.google.com/search?q={search_query}&tbs=cdr%3A1%2Ccd_min%3A{start_date[0]}%2F{start_date[2]}%2F{start_date[4:]}%2Ccd_max%3A{start_date[0]}%2F{start_date[2]}%2F{start_date[4:]}&tbm=nws\"\n",
    "    print(link)\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(link, headers=headers)\n",
    "    webpage = response.content\n",
    "    soup = BeautifulSoup(webpage, 'html5lib')\n",
    "    links = []\n",
    "    for div_tag in soup.find_all('div', class_='Gx5Zad'):\n",
    "        a_tag = div_tag.find('a')\n",
    "        if a_tag and 'href' in a_tag.attrs:\n",
    "            href = a_tag['href']\n",
    "            if href.startswith('/url?q='):\n",
    "                url = href.split('/url?q=')[1].split('&sa=')[0]\n",
    "                links.append(url)\n",
    "    return links\n",
    "\n",
    "# Example usage\n",
    "keyword = \"elon musk\"\n",
    "start_date = \"2024-06-04\"\n",
    "end_date = \"2024-06-04\"\n",
    "urls = find_url(keyword)\n",
    "for url in urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url(keyword):\n",
    "    start_date = \"6/8/2023\"    \n",
    "    root = \"https://www.google.com/\"\n",
    "    search_query = keyword.replace(\" \", \"+\")\n",
    "    link = f\"https://www.google.com/search?q={search_query}&tbs=cdr%3A1%2Ccd_min%3A{start_date[0]}%2F{start_date[2]}%2F{start_date[4:]}%2Ccd_max%3A{start_date[0]}%2F{start_date[2]}%2F{start_date[4:]}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(link, headers=headers)\n",
    "    webpage = response.content\n",
    "    soup = BeautifulSoup(webpage, 'html5lib')\n",
    "    links = []\n",
    "    # print(soup.find_all('div'))\n",
    "    # print(link)\n",
    "    \n",
    "    for div_tag in soup.find_all('div', class_='Gx5Zad'):\n",
    "        a_tag = div_tag.find('a')\n",
    "        if a_tag:\n",
    "            if 'href' in a_tag.attrs:\n",
    "                href = a_tag['href']\n",
    "                if href.startswith('/url?q='):\n",
    "                    url = href.split('/url?q=')[1].split('&sa=')[0]\n",
    "                    links.append(url)\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/search?q=elon+musk+news&tbs=cdr%3A1%2Ccd_min%3A6%2F8%2F2023%2Ccd_max%3A6%2F8%2F2023\n"
     ]
    }
   ],
   "source": [
    "a=find_url(\"elon musk news\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs:\n",
      "https://www.rediff.com/business/report/markets-at-fresh-highs-on-buying-in-hdfc-bank-ril/20240614.htm\n",
      "https://www.rediff.com/business/report/hero-motocorp-jsw-energy-among-7-midcap-stocks-likely-to-get-largecap-tag/20240614.htm\n",
      "https://www.indiaretailing.com/2024/06/14/stanley-lifestyles-eyes-rs-537-crore-via-ipo-issue-to-open-on-june-21/\n",
      "https://www.indiaretailing.com/2024/06/14/dollar-industries-sets-sights-on-southern-expansion-with-50-new-outlets/\n",
      "https://www.rediff.com/business/report/exports-up-9-in-may-trade-deficit-widens-to-7-month-high/20240614.htm\n",
      "https://twitter.com/Least_ordinary/status/1801577486075236861\n",
      "https://www.rediff.com/business/report/nse-chief-cautions-retail-investors-against-derivatives-trading/20240614.htm\n",
      "https://x.com/apoorv/status/1801524945983574180\n",
      "https://www.indiaretailing.com/2024/06/14/retail-tracker-may-store-openings/\n",
      "https://twitter.com/nimeshscnbc/status/1801448106560733403\n",
      "10 items total, printing the first one:\n",
      "Benchmark equity indices Sensex and Nifty hit their new closing lifetime highs on Friday, following buying in market heavyweights HDFC Bank, Reliance Industries and Mahindra & Mahindra amid encouraging export data.\n",
      "\n",
      "Rising for the third straight session, the 30-share BSE Sensex climbed 181.87 points or 0.24 per cent to settle at a new closing peak of 76,992.77.\n",
      "During the day, it jumped 270.4 points or 0.35 per cent to 77,081.30.\n",
      "The NSE Nifty rallied 66.70 points or 0.29 per cent to hit a record closing high of 23,465.60.\n",
      "Intra-day, it rose 91.5 points or 0.39 per cent to hit a fresh all-time high of 23,490.40.\n",
      "Among the 30 Sensex companies, Mahindra & Mahindra, Titan, HDFC Bank, Reliance Industries, UltraTech Cement, Bajaj Finance, Axis Bank, Tata Motors and Asian Paints were the biggest gainers.\n",
      "On the other hand, Tech Mahindra, Tata Consultancy Services, Wipro, HCL Technologies, Larsen & Toubro and State Bank of India were among the major laggards.\n",
      "India's merchandise exports in May 2024 rose by 9 per cent to $38.13 billion, from $34.95 billion in the year-ago month, according to government data released on Friday.\n",
      "Imports also increased by 7.7 per cent to $61.91 billion from $57.48 billion in May 2023.\n",
      "In Asian markets, Seoul, Tokyo and Shanghai settled higher, while Hong Kong ended lower.\n",
      "Foreign Institutional Investors (FIIs) offloaded equities worth Rs 3,033 crore on Thursday, according to exchange data.\n",
      "Global oil benchmark Brent crude dipped 0.12 per cent to $82.65 a barrel. Source: PTI © Copyright 2024 PTI. All rights reserved. Republication or redistribution of PTI content, including by framing or similar means, is expressly prohibited without the prior written consent.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from exa_py import Exa\n",
    "\n",
    "exa = Exa(api_key=\"9fccb5a3-0bff-453b-93bf-dd7e82f7a690\")\n",
    "\n",
    "one_week_ago = (datetime.now() - timedelta(days=7))\n",
    "date_cutoff = one_week_ago.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "search_query = \"hdfc stock latest news \"\n",
    "search_response = exa.search_and_contents(\n",
    "    search_query, use_autoprompt=True, start_published_date=date_cutoff,end_published_date=date_cutoff\n",
    ")\n",
    "\n",
    "urls = [result.url for result in search_response.results]\n",
    "print(\"URLs:\")\n",
    "for url in urls:\n",
    "    print(url)\n",
    "\n",
    "\n",
    "# result = exa.search_and_contents(\n",
    "#   \"HDFC stock latest news in a week\",\n",
    "#   type=\"neural\",\n",
    "#   use_autoprompt=True,\n",
    "#   num_results=10,\n",
    "#   text=True\n",
    "# )\n",
    "\n",
    "results = search_response.results\n",
    "result_item = results[0]\n",
    "print(f\"{len(results)} items total, printing the first one:\")\n",
    "print(result_item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import unquote\n",
    "\n",
    "def find_news_url(keyword, start_date, end_date):\n",
    "    root = \"https://www.google.com/\"\n",
    "    search_query = keyword.replace(\" \", \"+\")\n",
    "    link = f\"{root}search?q={search_query}&tbm=nws&tbs=cdr:1,cd_min:{start_date},cd_max:{end_date}\"\n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    \n",
    "    response = requests.get(link, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    news_links = []\n",
    "    \n",
    "    for article in soup.select('div.SoaBEf'):\n",
    "        link = article.select_one('a')\n",
    "        if link and 'href' in link.attrs:\n",
    "            url = link['href']\n",
    "            if url.startswith('/url?q='):\n",
    "                url = unquote(url.split('/url?q=')[1].split('&sa=')[0])\n",
    "            news_links.append(url)\n",
    "    \n",
    "    return news_links\n",
    "\n",
    "# Example usage\n",
    "keyword = \"elon musk\"\n",
    "start_date = \"6/8/2023\"\n",
    "end_date = \"6/8/2023\"\n",
    "results = find_news_url(keyword, start_date, end_date)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_article(url):\n",
    "    headers = {\n",
    "    'User-Agent': 'Your User Agent String',\n",
    "    }\n",
    "    r=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(r.text,'html.parser')\n",
    "    paragraphs=soup.find_all('p')\n",
    "    text= [paragraph.text for paragraph in paragraphs]\n",
    "    words=' '.join(text).split(' ')\n",
    "    article = ' '.join(words)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.motor1.com/news/702440/tesla-plaid-vs-dodge-demon-170-drag-race/\"\n",
    "a=scraping_article(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for 'byd': Negative\n",
      "Sentiment score: -0.53\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load spaCy for text processing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load RoBERTa sentiment analysis model\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"roberta-base\")\n",
    "\n",
    "def analyze_keyword_sentiment(text, keyword):\n",
    "    # Normalize text and keyword\n",
    "    text = text.lower()\n",
    "    keyword = keyword.lower()\n",
    "\n",
    "    # Check if keyword is in the text\n",
    "    if keyword not in text:\n",
    "        return \"Neutral (Keyword not found)\", 0\n",
    "\n",
    "    # Tokenize the text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Find sentences containing the keyword\n",
    "    relevant_sentences = [sent.text for sent in doc.sents if keyword in sent.text.lower()]\n",
    "    \n",
    "    if not relevant_sentences:\n",
    "        return \"Neutral (Keyword not in complete sentence)\", 0\n",
    "\n",
    "    # Analyze sentiment of relevant sentences\n",
    "    sentiments = [sentiment_analyzer(sent)[0] for sent in relevant_sentences]\n",
    "    \n",
    "    # Calculate weighted sentiment score\n",
    "    total_score = sum(sentiment['score'] if sentiment['label'] == 'POSITIVE' else -sentiment['score'] for sentiment in sentiments)\n",
    "    avg_score = total_score / len(sentiments)\n",
    "    \n",
    "    # Determine overall sentiment\n",
    "    if avg_score > 0.1:\n",
    "        label = \"Positive\"\n",
    "    elif avg_score < -0.1:\n",
    "        label = \"Negative\"\n",
    "    else:\n",
    "        label = \"Neutral\"\n",
    "\n",
    "    return label, avg_score\n",
    "\n",
    "# Example usage\n",
    "\n",
    "keyword = \"byd\"\n",
    "\n",
    "sentiment, score = analyze_keyword_sentiment(news_texts, keyword)\n",
    "print(f\"Sentiment for '{keyword}': {sentiment}\")\n",
    "print(f\"Sentiment score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
