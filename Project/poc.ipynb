{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import yfinance as yf\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import unquote\n",
    "from scipy.special import softmax\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_token=os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "os.environ['HF_TOKEN']=api_token\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']=api_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_sector(stock_symbol):\n",
    "    try:\n",
    "        stock = yf.Ticker(stock_symbol)\n",
    "        info = stock.info\n",
    "        sector = info.get('sector', 'Sector information not available')\n",
    "        industry=info.get('industry','Industry information not available')        \n",
    "        return sector,industry\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock: AAPL\n",
      "Sector: Technology\n",
      "Industry: Consumer Electronics\n"
     ]
    }
   ],
   "source": [
    "stock_symbol = input(\"Enter Stock Symbol: \")\n",
    "sector,industry = get_stock_sector(stock_symbol)\n",
    "if sector and industry:\n",
    "    print(\"Stock:\",stock_symbol)\n",
    "    print(\"Sector:\", sector)\n",
    "    print('Industry:',industry)\n",
    "elif sector:\n",
    "    print(\"Stock:\",stock_symbol)\n",
    "    print(\"Sector:\", sector)\n",
    "    print('Industry:',\"Falied to retrive industry information\")\n",
    "elif industry:\n",
    "    print(\"Stock:\",stock_symbol)\n",
    "    print(\"Sector:\", \"Failed to retrieve sector information.\")\n",
    "    print('Industry:',industry)   \n",
    "else:\n",
    "    print(\"Failed to retrieve sector information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_article(url):\n",
    "    headers = {\n",
    "    'User-Agent': 'Your User Agent String',\n",
    "    }\n",
    "    r=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(r.text,'html.parser')\n",
    "    paragraphs=soup.find_all('p')\n",
    "    text= [paragraph.text for paragraph in paragraphs]\n",
    "    words=' '.join(text).split(' ')\n",
    "    article = ' '.join(words)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url(keyword):\n",
    "    \n",
    "    root = \"https://www.google.com/\"\n",
    "    search_query = keyword.replace(\" \", \"+\")\n",
    "    link = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(link, headers=headers)\n",
    "    webpage = response.content\n",
    "    soup = BeautifulSoup(webpage, 'html5lib')\n",
    "    links = []\n",
    "    for div_tag in soup.find_all('div', class_='Gx5Zad'):\n",
    "        a_tag = div_tag.find('a')\n",
    "        if a_tag:\n",
    "            if 'href' in a_tag.attrs:\n",
    "                href = a_tag['href']\n",
    "                if href.startswith('/url?q='):\n",
    "                    url = href.split('/url?q=')[1].split('&sa=')[0]\n",
    "                    links.append(url)\n",
    "    return links    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_chunks(data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=3000,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    docs=text_splitter.split_text(data)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bart_model(model_name=\"facebook/bart-large-cnn\"):\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_news_url(keyword, start_date, end_date):\n",
    "    root = \"https://www.google.com/\"\n",
    "    search_query = keyword.replace(\" \", \"+\")\n",
    "    link = f\"{root}search?q={search_query}&tbm=nws&tbs=cdr:1,cd_min:{start_date},cd_max:{end_date}\"\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "\n",
    "    response = requests.get(link, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    news_links = []\n",
    "\n",
    "    for article in soup.select('div.SoaBEf'):\n",
    "        link = article.select_one('a')\n",
    "        if link and 'href' in link.attrs:\n",
    "            url = link['href']\n",
    "            if url.startswith('/url?q='):\n",
    "                url = unquote(url.split('/url?q=')[1].split('&sa=')[0])\n",
    "            news_links.append(url)\n",
    "\n",
    "    return news_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(tokenizer, model, text, max_chunk_length, summary_max_length):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_chunk_length, truncation=True)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=summary_max_length, min_length=200, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_article(url, model_name=\"facebook/bart-large-cnn\"):\n",
    "    data = scraping_article(url)\n",
    "    chunks = to_chunks(data)\n",
    "    # tokenizer, model=load_pegasus_model(\"google/pegasus-xsum\")\n",
    "    tokenizer, model = load_bart_model(model_name)\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk\n",
    "        summary = summarize_text(tokenizer, model, chunk_text,3000,800)\n",
    "        summaries.append(summary)\n",
    "    concatenated_summaries = \" \".join(summaries)\n",
    "    #  Second summarization pass: Summarize the concatenated summaries\n",
    "    intermediate_chunks = [concatenated_summaries[i:i+3000] for i in range(0, len(concatenated_summaries), 3000)]\n",
    "    final_summaries = []\n",
    "    for intermediate_chunk in intermediate_chunks:\n",
    "        final_summary = summarize_text(tokenizer, model, intermediate_chunk,3000,800)\n",
    "        final_summaries.append(final_summary)    \n",
    "    final_summary_text = \" \".join(final_summaries)    \n",
    "    return final_summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk, CEO, Tesla, first disclosed the intention to reduce the companyâ€™s workforce by over 10 per cent on 15 April. Since then, multiple rounds of layoffs have been executed, with Musk reportedly aiming for a 20 per cent reduction in headcount. The latest round of layoffs will primarily impact workers at Tesla's Palo Alto and Fremont facilities. The termination process set to commence within a 14-day period starting 20 June, 2024, according to the Worker Adjustment and Retraining Notification (WARN) notice issued by Tesla. Last month, Tesla announced plans to cut 6,020 jobs in California and Texas as part of its broader downsizing strategy. The move also aligns with broader trends in the tech and automotive industries, where companies prioritise cost management and operational efficiency. For Tesla, maintaining market leadership requires a balance between innovation and financial prudence. The company says it is committed to being a leader in the electric-vehicle industry and will continue to innovate.\n"
     ]
    }
   ],
   "source": [
    "# url=find_url('elon musk')[0]\n",
    "url=\"https://www.hrkatha.com/global-hr-news/tesla-to-lay-off-601-more-employees-amidst-market-challenges/\"\n",
    "summary = summarize_article(url)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti_model(model_name=\"cardiffnlp/twitter-roberta-base-sentiment\"):\n",
    "    tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    return tokenizer,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_senti(news_texts):\n",
    "    tokenizer,model=senti_model()\n",
    "    encoded=tokenizer(news_texts,return_tensors='pt')\n",
    "    output=model(**encoded)\n",
    "    scores=output[0][0].detach().numpy()\n",
    "    scores=softmax(scores)\n",
    "    weights = {\n",
    "        'neg': -1,\n",
    "        'neu': 0,\n",
    "        'pos': 1\n",
    "    }\n",
    "    probabilities = {\n",
    "        'neg': scores[0],\n",
    "        'neu': scores[1],\n",
    "        'pos': scores[2]\n",
    "    }\n",
    "    compound_score = sum(probabilities[label] * weights[label] for label in probabilities)\n",
    "    senti_dict={\n",
    "        'neg':scores[0],\n",
    "        'neu': scores[1],\n",
    "        'pos': scores[2],\n",
    "        'polarity':compound_score        \n",
    "    }\n",
    "    return senti_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjectivity_analysis(text):\n",
    "    blob = TextBlob(text)\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    return subjectivity\n",
    "\n",
    "def relevance_scoring(summary, stock_name, domain_keywords):\n",
    "    relevance_score = sum(1 for word in domain_keywords if word in summary) + (stock_name in summary)\n",
    "    return relevance_score\n",
    "\n",
    "def urgency_scoring(summary):\n",
    "    # Look for time-sensitive words or phrases in the summary\n",
    "    urgency_keywords = ['breaking', 'urgent', 'immediate']\n",
    "    urgency_score = sum(1 for word in urgency_keywords if word in summary)\n",
    "    return urgency_score\n",
    "\n",
    "def mention_count(summary, stock_name, domain_keywords):\n",
    "    # Count how many times the stock name and domain keywords appear in the summary\n",
    "    count = sum(summary.count(word) for word in domain_keywords) + summary.count(stock_name)\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(summary, stock_name, domain_keywords):\n",
    "    # Sentiment analysis\n",
    "    sentiment_scores = find_senti(summary)\n",
    "    subjectivity_score = subjectivity_analysis(summary)\n",
    "    \n",
    "    # Financial analysis\n",
    "    relevance_score = relevance_scoring(summary, stock_name, domain_keywords)\n",
    "    urgency_score = urgency_scoring(summary)\n",
    "    mention_count_score = mention_count(summary, stock_name, domain_keywords)\n",
    "    \n",
    "    # Construct feature vector\n",
    "    features = {\n",
    "        'compound_sentiment_score': sentiment_scores['polarity'],\n",
    "        'subjectivity_score': subjectivity_score,\n",
    "        'relevance_score': relevance_score,\n",
    "        'urgency_score': urgency_score,\n",
    "        'mention_count': mention_count_score,\n",
    "        'negative_sentiment_score': sentiment_scores['neg'],\n",
    "        'neutral_sentiment_score': sentiment_scores['neu'],\n",
    "        'positive_sentiment_score': sentiment_scores['pos']\n",
    "    }\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utkar\\Desktop\\DG_liger\\Project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'compound_sentiment_score': 0.5478894338011742, 'subjectivity_score': 0.5444444444444444, 'relevance_score': 10, 'urgency_score': 1, 'mention_count': 355, 'negative_sentiment_score': 0.008158185, 'neutral_sentiment_score': 0.43579414, 'positive_sentiment_score': 0.5560476}\n"
     ]
    }
   ],
   "source": [
    "scores=extract_features(summary,\"tesla\",\"technology\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://timesofindia.indiatimes.com/technology/tech-news/elon-musk-has-a-clarification-for-go-fk-yourselves-comment/articleshow/111167837.cms\n"
     ]
    }
   ],
   "source": [
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=scraping_article(\"https://www.motor1.com/news/702440/tesla-plaid-vs-dodge-demon-170-drag-race/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
