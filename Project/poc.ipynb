{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import yfinance as yf\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from scipy.special import softmax\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_token=os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "os.environ['HF_TOKEN']=api_token\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']=api_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_sector(stock_symbol):\n",
    "    try:\n",
    "        stock = yf.Ticker(stock_symbol)\n",
    "        info = stock.info\n",
    "        sector = info.get('sector', 'Sector information not available')\n",
    "        industry=info.get('industry','Industry information not available')        \n",
    "        return sector,industry\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock: AAPL\n",
      "Sector: Technology\n",
      "Industry: Consumer Electronics\n"
     ]
    }
   ],
   "source": [
    "stock_symbol = input(\"Enter Stock Symbol: \")\n",
    "sector,industry = get_stock_sector(stock_symbol)\n",
    "if sector and industry:\n",
    "    print(\"Stock:\",stock_symbol)\n",
    "    print(\"Sector:\", sector)\n",
    "    print('Industry:',industry)\n",
    "elif sector:\n",
    "    print(\"Stock:\",stock_symbol)\n",
    "    print(\"Sector:\", sector)\n",
    "    print('Industry:',\"Falied to retrive industry information\")\n",
    "elif industry:\n",
    "    print(\"Stock:\",stock_symbol)\n",
    "    print(\"Sector:\", \"Failed to retrieve sector information.\")\n",
    "    print('Industry:',industry)   \n",
    "else:\n",
    "    print(\"Failed to retrieve sector information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_article(url):\n",
    "    headers = {\n",
    "    'User-Agent': 'Your User Agent String',\n",
    "    }\n",
    "    r=requests.get(url,headers=headers)\n",
    "    soup=BeautifulSoup(r.text,'html.parser')\n",
    "    paragraphs=soup.find_all('p')\n",
    "    text= [paragraph.text for paragraph in paragraphs]\n",
    "    words=' '.join(text).split(' ')\n",
    "    article = ' '.join(words)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url(keyword):\n",
    "    \n",
    "    root = \"https://www.google.com/\"\n",
    "    search_query = keyword.replace(\" \", \"+\")\n",
    "    link = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(link, headers=headers)\n",
    "    webpage = response.content\n",
    "    soup = BeautifulSoup(webpage, 'html5lib')\n",
    "    links = []\n",
    "    for div_tag in soup.find_all('div', class_='Gx5Zad'):\n",
    "        a_tag = div_tag.find('a')\n",
    "        if a_tag:\n",
    "            if 'href' in a_tag.attrs:\n",
    "                href = a_tag['href']\n",
    "                if href.startswith('/url?q='):\n",
    "                    url = href.split('/url?q=')[1].split('&sa=')[0]\n",
    "                    links.append(url)\n",
    "    return links    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_chunks(data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=3000,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    docs=text_splitter.split_text(data)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bart_model(model_name=\"facebook/bart-large-cnn\"):\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(tokenizer, model, text, max_chunk_length, summary_max_length):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_chunk_length, truncation=True)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=summary_max_length, min_length=200, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_article(url, model_name=\"facebook/bart-large-cnn\"):\n",
    "    data = scraping_article(url)\n",
    "    chunks = to_chunks(data)\n",
    "    tokenizer, model = load_bart_model(model_name)\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk\n",
    "        summary = summarize_text(tokenizer, model, chunk_text,3000,800)\n",
    "        summaries.append(summary)\n",
    "    concatenated_summaries = \" \".join(summaries)\n",
    "    #  Second summarization pass: Summarize the concatenated summaries\n",
    "    intermediate_chunks = [concatenated_summaries[i:i+3000] for i in range(0, len(concatenated_summaries), 3000)]\n",
    "    final_summaries = []\n",
    "    for intermediate_chunk in intermediate_chunks:\n",
    "        final_summary = summarize_text(tokenizer, model, intermediate_chunk,3000,800)\n",
    "        final_summaries.append(final_summary)    \n",
    "    final_summary_text = \" \".join(final_summaries)    \n",
    "    return final_summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dubai Police in November 2019 hinted that it could add Tesla Cybertruck to its impressive fleet of cars. The police already included Bugattis, Aston Martins, and Porsches. Elon Musk, co-founder and CEO of Tesla, soon dropped a comment on the post. “This is awesome and should come as no surprise. Dubai makes wonderful decisions,’ wrote X user Cindys, while another called it “Smart and wise choice” “Awesome. Need them in Chicago,” said a third, while a fourth said it was “Fantastic seeing the powerful Dubai Police add Cybert truck! Elon Musk’s electric truck is going across the globe and joined the most innovative police force! Bravo!” joined a fifth individual. ” Combining cutting-edge technology with sustainability sets a great example for modern policing,“ posted X user Sheikh Aaqib Satar. ‘This is a great decision.\n"
     ]
    }
   ],
   "source": [
    "url=find_url('elon musk')[1]\n",
    "summary = summarize_article(url)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti_model(model_name=\"cardiffnlp/twitter-roberta-base-sentiment\"):\n",
    "    tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    return tokenizer,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_senti(news_texts):\n",
    "    tokenizer,model=senti_model()\n",
    "    encoded=tokenizer(news_texts,return_tensors='pt')\n",
    "    output=model(**encoded)\n",
    "    scores=output[0][0].detach().numpy()\n",
    "    scores=softmax(scores)\n",
    "    weights = {\n",
    "        'neg': -1,\n",
    "        'neu': 0,\n",
    "        'pos': 1\n",
    "    }\n",
    "    probabilities = {\n",
    "        'neg': scores[0],\n",
    "        'neu': scores[1],\n",
    "        'pos': scores[2]\n",
    "    }\n",
    "    compound_score = sum(probabilities[label] * weights[label] for label in probabilities)\n",
    "    senti_dict={\n",
    "        'neg':scores[0],\n",
    "        'neu': scores[1],\n",
    "        'pos': scores[2],\n",
    "        'polarity':compound_score        \n",
    "    }\n",
    "    return senti_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjectivity_analysis(text):\n",
    "    blob = TextBlob(text)\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    return subjectivity\n",
    "\n",
    "def relevance_scoring(summary, stock_name, domain_keywords):\n",
    "    relevance_score = sum(1 for word in domain_keywords if word in summary) + (stock_name in summary)\n",
    "    return relevance_score\n",
    "\n",
    "def urgency_scoring(summary):\n",
    "    # Look for time-sensitive words or phrases in the summary\n",
    "    urgency_keywords = ['breaking', 'urgent', 'immediate']\n",
    "    urgency_score = sum(1 for word in urgency_keywords if word in summary)\n",
    "    return urgency_score\n",
    "\n",
    "def mention_count(summary, stock_name, domain_keywords):\n",
    "    # Count how many times the stock name and domain keywords appear in the summary\n",
    "    count = sum(summary.count(word) for word in domain_keywords) + summary.count(stock_name)\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(summary, stock_name, domain_keywords):\n",
    "    # Sentiment analysis\n",
    "    sentiment_scores = find_senti(summary)\n",
    "    subjectivity_score = subjectivity_analysis(summary)\n",
    "    \n",
    "    # Financial analysis\n",
    "    relevance_score = relevance_scoring(summary, stock_name, domain_keywords)\n",
    "    urgency_score = urgency_scoring(summary)\n",
    "    mention_count_score = mention_count(summary, stock_name, domain_keywords)\n",
    "    \n",
    "    # Construct feature vector\n",
    "    features = {\n",
    "        'compound_sentiment_score': sentiment_scores['polarity'],\n",
    "        'subjectivity_score': subjectivity_score,\n",
    "        'relevance_score': relevance_score,\n",
    "        'urgency_score': urgency_score,\n",
    "        'mention_count': mention_count_score,\n",
    "        'negative_sentiment_score': sentiment_scores['neg'],\n",
    "        'neutral_sentiment_score': sentiment_scores['neu'],\n",
    "        'positive_sentiment_score': sentiment_scores['pos']\n",
    "    }\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'compound_sentiment_score': 0.9574133579153568, 'subjectivity_score': 0.6964285714285715, 'relevance_score': 10, 'urgency_score': 0, 'mention_count': 352, 'negative_sentiment_score': 0.0023002203, 'neutral_sentiment_score': 0.03798616, 'positive_sentiment_score': 0.9597136}\n"
     ]
    }
   ],
   "source": [
    "scores=extract_features(summary,\"tesla\",\"technology\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
